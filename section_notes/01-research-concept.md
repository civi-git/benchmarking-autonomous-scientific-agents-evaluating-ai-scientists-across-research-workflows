Design a benchmark to evaluate how well different AI agents (e.g., AutoGPT, ChemCrow, SciAgent, etc.) perform key tasks in the scientific research pipeline â€” such as literature review, hypothesis generation, experiment planning, data analysis, and drafting publications.

The goal is to compare performance across agents and establish a standardized framework to assess scientific capability in AI systems.

